<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 4 Regression | Data Analysis and Visualization - Practicals</title>
  <meta name="description" content="This gitbook contains all practical assignments for the course Data Analysis and Visualization." />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 4 Regression | Data Analysis and Visualization - Practicals" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This gitbook contains all practical assignments for the course Data Analysis and Visualization." />
  <meta name="github-repo" content="thomvolker/dav" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 4 Regression | Data Analysis and Visualization - Practicals" />
  
  <meta name="twitter:description" content="This gitbook contains all practical assignments for the course Data Analysis and Visualization." />
  

<meta name="author" content="Thom Volker" />


<meta name="date" content="2021-02-03" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="ggplot2.html"/>
<link rel="next" href="applications.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>


<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">A Minimal Book Example</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Introduction</a></li>
<li class="chapter" data-level="2" data-path="eda.html"><a href="eda.html"><i class="fa fa-check"></i><b>2</b> Week 1 - Data manipulation and EDA</a><ul>
<li class="chapter" data-level="2.1" data-path="eda.html"><a href="eda.html#introduction-1"><i class="fa fa-check"></i><b>2.1</b> Introduction</a></li>
<li class="chapter" data-level="2.2" data-path="eda.html"><a href="eda.html#data-types"><i class="fa fa-check"></i><b>2.2</b> Data types</a><ul>
<li class="chapter" data-level="2.2.1" data-path="eda.html"><a href="eda.html#exercise-1"><i class="fa fa-check"></i><b>2.2.1</b> Exercise 1</a></li>
<li class="chapter" data-level="2.2.2" data-path="eda.html"><a href="eda.html#exercise-2"><i class="fa fa-check"></i><b>2.2.2</b> Exercise 2</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="eda.html"><a href="eda.html#lists-and-data-frames"><i class="fa fa-check"></i><b>2.3</b> Lists and data frames</a><ul>
<li class="chapter" data-level="2.3.1" data-path="eda.html"><a href="eda.html#exercise-3"><i class="fa fa-check"></i><b>2.3.1</b> Exercise 3</a></li>
<li class="chapter" data-level="2.3.2" data-path="eda.html"><a href="eda.html#exercise-4"><i class="fa fa-check"></i><b>2.3.2</b> Exercise 4</a></li>
<li class="chapter" data-level="2.3.3" data-path="eda.html"><a href="eda.html#exercise-5"><i class="fa fa-check"></i><b>2.3.3</b> Exercise 5</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="eda.html"><a href="eda.html#loading-viewing-and-summarising-data"><i class="fa fa-check"></i><b>2.4</b> Loading, viewing and summarising data</a><ul>
<li class="chapter" data-level="2.4.1" data-path="eda.html"><a href="eda.html#exercise-6"><i class="fa fa-check"></i><b>2.4.1</b> Exercise 6</a></li>
<li class="chapter" data-level="2.4.2" data-path="eda.html"><a href="eda.html#exercise-7"><i class="fa fa-check"></i><b>2.4.2</b> Exercise 7</a></li>
<li class="chapter" data-level="2.4.3" data-path="eda.html"><a href="eda.html#exercise-8"><i class="fa fa-check"></i><b>2.4.3</b> Exercise 8</a></li>
<li class="chapter" data-level="2.4.4" data-path="eda.html"><a href="eda.html#exercise-9"><i class="fa fa-check"></i><b>2.4.4</b> Exercise 9</a></li>
<li class="chapter" data-level="2.4.5" data-path="eda.html"><a href="eda.html#exercise-10"><i class="fa fa-check"></i><b>2.4.5</b> Exercise 10</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="eda.html"><a href="eda.html#data-transformation-with-dplyr"><i class="fa fa-check"></i><b>2.5</b> Data transformation with <code>dplyr</code></a><ul>
<li class="chapter" data-level="2.5.1" data-path="eda.html"><a href="eda.html#exercise-11"><i class="fa fa-check"></i><b>2.5.1</b> Exercise 11</a></li>
<li class="chapter" data-level="2.5.2" data-path="eda.html"><a href="eda.html#exercise-12"><i class="fa fa-check"></i><b>2.5.2</b> Exercise 12</a></li>
<li class="chapter" data-level="2.5.3" data-path="eda.html"><a href="eda.html#exercise-13"><i class="fa fa-check"></i><b>2.5.3</b> Exercise 13</a></li>
<li class="chapter" data-level="2.5.4" data-path="eda.html"><a href="eda.html#exercise-14"><i class="fa fa-check"></i><b>2.5.4</b> Exercise 14</a></li>
<li class="chapter" data-level="2.5.5" data-path="eda.html"><a href="eda.html#exercise-15"><i class="fa fa-check"></i><b>2.5.5</b> Exercise 15</a></li>
<li class="chapter" data-level="2.5.6" data-path="eda.html"><a href="eda.html#exercise-16"><i class="fa fa-check"></i><b>2.5.6</b> Exercise 16</a></li>
<li class="chapter" data-level="2.5.7" data-path="eda.html"><a href="eda.html#exercise-17"><i class="fa fa-check"></i><b>2.5.7</b> Exercise 17</a></li>
<li class="chapter" data-level="2.5.8" data-path="eda.html"><a href="eda.html#exercise-18"><i class="fa fa-check"></i><b>2.5.8</b> Exercise 18</a></li>
<li class="chapter" data-level="2.5.9" data-path="eda.html"><a href="eda.html#exercise-19"><i class="fa fa-check"></i><b>2.5.9</b> Exercise 19</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="ggplot2.html"><a href="ggplot2.html"><i class="fa fa-check"></i><b>3</b> Data Visualization using ggplot2</a><ul>
<li class="chapter" data-level="3.1" data-path="ggplot2.html"><a href="ggplot2.html#introduction-2"><i class="fa fa-check"></i><b>3.1</b> Introduction</a><ul>
<li class="chapter" data-level="3.1.1" data-path="ggplot2.html"><a href="ggplot2.html#exercise-1-1"><i class="fa fa-check"></i><b>3.1.1</b> Exercise 1</a></li>
<li class="chapter" data-level="3.1.2" data-path="ggplot2.html"><a href="ggplot2.html#exercise-2-1"><i class="fa fa-check"></i><b>3.1.2</b> Exercise 2</a></li>
<li class="chapter" data-level="3.1.3" data-path="ggplot2.html"><a href="ggplot2.html#exercise-3-1"><i class="fa fa-check"></i><b>3.1.3</b> Exercise 3</a></li>
<li class="chapter" data-level="3.1.4" data-path="ggplot2.html"><a href="ggplot2.html#exercise-4-1"><i class="fa fa-check"></i><b>3.1.4</b> Exercise 4</a></li>
<li class="chapter" data-level="3.1.5" data-path="ggplot2.html"><a href="ggplot2.html#exercise-5-1"><i class="fa fa-check"></i><b>3.1.5</b> Exercise 5</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="ggplot2.html"><a href="ggplot2.html#visual-exploratory-data-analysis"><i class="fa fa-check"></i><b>3.2</b> Visual exploratory data analysis</a><ul>
<li class="chapter" data-level="3.2.1" data-path="ggplot2.html"><a href="ggplot2.html#exercise-6-1"><i class="fa fa-check"></i><b>3.2.1</b> Exercise 6</a></li>
<li class="chapter" data-level="3.2.2" data-path="ggplot2.html"><a href="ggplot2.html#exercise-7-1"><i class="fa fa-check"></i><b>3.2.2</b> Exercise 7</a></li>
<li class="chapter" data-level="3.2.3" data-path="ggplot2.html"><a href="ggplot2.html#exercise-8-1"><i class="fa fa-check"></i><b>3.2.3</b> Exercise 8</a></li>
<li class="chapter" data-level="3.2.4" data-path="ggplot2.html"><a href="ggplot2.html#exercise-9-1"><i class="fa fa-check"></i><b>3.2.4</b> Exercise 9</a></li>
<li class="chapter" data-level="3.2.5" data-path="ggplot2.html"><a href="ggplot2.html#exercise-10-1"><i class="fa fa-check"></i><b>3.2.5</b> Exercise 10</a></li>
<li class="chapter" data-level="3.2.6" data-path="ggplot2.html"><a href="ggplot2.html#exercise-11-1"><i class="fa fa-check"></i><b>3.2.6</b> Exercise 11</a></li>
<li class="chapter" data-level="3.2.7" data-path="ggplot2.html"><a href="ggplot2.html#exercise-12-1"><i class="fa fa-check"></i><b>3.2.7</b> Exercise 12</a></li>
<li class="chapter" data-level="3.2.8" data-path="ggplot2.html"><a href="ggplot2.html#exercise-13-1"><i class="fa fa-check"></i><b>3.2.8</b> Exercise 13</a></li>
<li class="chapter" data-level="3.2.9" data-path="ggplot2.html"><a href="ggplot2.html#exercise-14---17"><i class="fa fa-check"></i><b>3.2.9</b> Exercise 14 - 17</a></li>
<li class="chapter" data-level="3.2.10" data-path="ggplot2.html"><a href="ggplot2.html#exercise-18-1"><i class="fa fa-check"></i><b>3.2.10</b> Exercise 18</a></li>
<li class="chapter" data-level="3.2.11" data-path="ggplot2.html"><a href="ggplot2.html#exercise-19-1"><i class="fa fa-check"></i><b>3.2.11</b> Exercise 19</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="reg.html"><a href="reg.html"><i class="fa fa-check"></i><b>4</b> Regression</a><ul>
<li class="chapter" data-level="4.1" data-path="reg.html"><a href="reg.html#book"><i class="fa fa-check"></i><b>4.1</b> Book</a><ul>
<li class="chapter" data-level="4.1.1" data-path="reg.html"><a href="reg.html#chapter-3"><i class="fa fa-check"></i><b>4.1.1</b> Chapter 3</a></li>
<li class="chapter" data-level="4.1.2" data-path="reg.html"><a href="reg.html#chapter-6---model-selection"><i class="fa fa-check"></i><b>4.1.2</b> Chapter 6 - Model selection</a></li>
<li class="chapter" data-level="4.1.3" data-path="reg.html"><a href="reg.html#chapter-7---shrinkage-methods"><i class="fa fa-check"></i><b>4.1.3</b> Chapter 7 - Shrinkage methods</a></li>
<li class="chapter" data-level="4.1.4" data-path="reg.html"><a href="reg.html#resampling-methods"><i class="fa fa-check"></i><b>4.1.4</b> Resampling Methods</a></li>
<li class="chapter" data-level="4.1.5" data-path="reg.html"><a href="reg.html#moving-beyond-linearity"><i class="fa fa-check"></i><b>4.1.5</b> Moving beyond linearity</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="reg.html"><a href="reg.html#lecture---regression-i"><i class="fa fa-check"></i><b>4.2</b> Lecture - Regression I</a><ul>
<li class="chapter" data-level="4.2.1" data-path="reg.html"><a href="reg.html#model-accuracy-1"><i class="fa fa-check"></i><b>4.2.1</b> Model accuracy</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="reg.html"><a href="reg.html#lecture---regression-ii"><i class="fa fa-check"></i><b>4.3</b> Lecture - Regression II</a><ul>
<li class="chapter" data-level="4.3.1" data-path="reg.html"><a href="reg.html#feature-selection-penalization"><i class="fa fa-check"></i><b>4.3.1</b> Feature selection / penalization</a></li>
<li class="chapter" data-level="4.3.2" data-path="reg.html"><a href="reg.html#wrapper-methods"><i class="fa fa-check"></i><b>4.3.2</b> Wrapper methods</a></li>
<li class="chapter" data-level="4.3.3" data-path="reg.html"><a href="reg.html#filter-methods"><i class="fa fa-check"></i><b>4.3.3</b> Filter methods</a></li>
<li class="chapter" data-level="4.3.4" data-path="reg.html"><a href="reg.html#embedded-methods"><i class="fa fa-check"></i><b>4.3.4</b> Embedded methods</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="reg.html"><a href="reg.html#lecture---regression-iii"><i class="fa fa-check"></i><b>4.4</b> Lecture - Regression III</a><ul>
<li class="chapter" data-level="4.4.1" data-path="reg.html"><a href="reg.html#polynomial-regression-1"><i class="fa fa-check"></i><b>4.4.1</b> Polynomial regression</a></li>
<li class="chapter" data-level="4.4.2" data-path="reg.html"><a href="reg.html#piecewise-regression"><i class="fa fa-check"></i><b>4.4.2</b> Piecewise regression</a></li>
<li class="chapter" data-level="4.4.3" data-path="reg.html"><a href="reg.html#basis-functions-1"><i class="fa fa-check"></i><b>4.4.3</b> Basis functions</a></li>
<li class="chapter" data-level="4.4.4" data-path="reg.html"><a href="reg.html#splines"><i class="fa fa-check"></i><b>4.4.4</b> Splines</a></li>
<li class="chapter" data-level="4.4.5" data-path="reg.html"><a href="reg.html#local-regression-1"><i class="fa fa-check"></i><b>4.4.5</b> Local regression</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="reg.html"><a href="reg.html#practical-regression-i"><i class="fa fa-check"></i><b>4.5</b> Practical Regression I</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="applications.html"><a href="applications.html"><i class="fa fa-check"></i><b>5</b> Applications</a><ul>
<li class="chapter" data-level="5.1" data-path="applications.html"><a href="applications.html#example-one"><i class="fa fa-check"></i><b>5.1</b> Example one</a></li>
<li class="chapter" data-level="5.2" data-path="applications.html"><a href="applications.html#example-two"><i class="fa fa-check"></i><b>5.2</b> Example two</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="final-words.html"><a href="final-words.html"><i class="fa fa-check"></i><b>6</b> Final Words</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Data Analysis and Visualization - Practicals</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="reg" class="section level1">
<h1><span class="header-section-number">Chapter 4</span> Regression</h1>
<div id="book" class="section level2">
<h2><span class="header-section-number">4.1</span> Book</h2>
<div id="chapter-3" class="section level3">
<h3><span class="header-section-number">4.1.1</span> Chapter 3</h3>
<div id="prediction" class="section level4">
<h4><span class="header-section-number">4.1.1.1</span> Prediction</h4>
<p>In many cases, predictors <span class="math inline">\(X\)</span> are readily available, but <span class="math inline">\(Y\)</span> is not. We can however predict <span class="math inline">\(Y\)</span>, as the error term <span class="math inline">\(\epsilon_i\)</span> averages to zero. Often, <span class="math inline">\(\hat{f}\)</span> is treated as a black box when prediction is of interest, as one is typically not concerned with the exact form of <span class="math inline">\(\hat{f}\)</span>, provided that it yields accurate predictions for <span class="math inline">\(Y\)</span>.</p>
<ul>
<li>Reducible error: the misspecification error that is due to the fact that <span class="math inline">\(\hat{f}\)</span> is usually not a perfect estimate of <span class="math inline">\(f\)</span>, that can potentially be improved.</li>
<li>Irreducible error: <span class="math inline">\(Y\)</span> is not only a function of <span class="math inline">\(f\)</span>, but also of <span class="math inline">\(\epsilon\)</span>; and the variability associated with <span class="math inline">\(\epsilon\)</span> affects our predictions. <span class="math inline">\(\epsilon\)</span> may contain unmeasured variables and/or unmeasured variation.
<ul>
<li>Given that <span class="math inline">\(\hat{Y} = \hat{f}(X)\)</span>, assume that <span class="math inline">\(\hat{f}\)</span> and <span class="math inline">\(X\)</span> are fixed.
<span class="math display">\[\begin{align} E(Y - \hat{Y})^2 &amp;= E[f(X) + \epsilon - \hat{f}(X)]^2 \\
&amp;= [f(X) - \hat{f}(X)]^2 + Var(\epsilon)
\end{align}\]</span>
<span class="math inline">\(E(Y - \hat{Y})^2\)</span> represents the expected value of the squared difference between the predicted and actual value of <span class="math inline">\(Y\)</span> and <span class="math inline">\(Var(\epsilon)\)</span> the variability associated with <span class="math inline">\(\epsilon\)</span></li>
<li>The irreducible error provides the upper bound of the accuracy of the predictions, but this upper bound is generally unknown in practice.</li>
</ul></li>
</ul>
</div>
<div id="inference" class="section level4">
<h4><span class="header-section-number">4.1.1.2</span> Inference</h4>
<p>The goal of inference is to understand how <span class="math inline">\(Y\)</span> changes as a function of <span class="math inline">\(X\)</span>. If this is the case, <span class="math inline">\(\hat{f}\)</span> cannot be treated as a black box, because we need to know its exact form. Questions of interest can be:</p>
<ul>
<li>Which predictors are associated with <span class="math inline">\(Y\)</span>?</li>
<li>What is the relation between <span class="math inline">\(Y\)</span> and each predictor?</li>
<li>Can the relationship between <span class="math inline">\(Y\)</span> and each predictor be adequately summarized with a linear equation?</li>
</ul>
<p>We want to find a function <span class="math inline">\(\hat{f}\)</span> such that <span class="math inline">\(Y \approx \hat{f}(X)\)</span> for any observation <span class="math inline">\((X, Y)\)</span>.</p>
</div>
<div id="parametric-methods" class="section level4">
<h4><span class="header-section-number">4.1.1.3</span> Parametric methods</h4>
<ul>
<li>Parametric methods make an assumptions about the functional form of <span class="math inline">\(f\)</span>.</li>
<li>After selecting a global model, we need a procedure that trains the model (in linear regression, this refers to estimating the parameters <span class="math inline">\(\beta_0, \beta_1, \dots, \beta_p\)</span>). Parametric approaches reduce the problem of estimating <span class="math inline">\(f\)</span> down to one of estimating a set of parameters.</li>
</ul>
<p>Assuming a parametric form for <span class="math inline">\(f\)</span> simplifies the problem of estimating <span class="math inline">\(f\)</span> as it generally is much easier to estimate a set of parameters than it is to fit an entirely arbitrary function. However, a potential drawback is that the model considered in a parametric approach usually not matches the true unknown form of <span class="math inline">\(f\)</span>. This can be partly overcome by fitting more flexible models, which in turn might lead to overfitting.</p>
</div>
<div id="non-parametric-approaches" class="section level4">
<h4><span class="header-section-number">4.1.1.4</span> Non-parametric approaches</h4>
<ul>
<li>Seek an estimate that is as close to <span class="math inline">\(f\)</span> as possible without overfitting. Non-parametric approaches usually do not make assumptions about the functional form of <span class="math inline">\(f\)</span>. However, they generally need a much larger number of observations relative to parametric methods to obtain an accurate estimate of <span class="math inline">\(f\)</span>.</li>
</ul>
</div>
<div id="statistical-learning" class="section level4">
<h4><span class="header-section-number">4.1.1.5</span> Statistical learning</h4>
<p><img src="dav_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
<p>Restrictive models are much more interpretable. Very flexible approaches may lead to such complicated estimates of <span class="math inline">\(f\)</span> that it is difficult to understand how any individual predictor is associated with the response. For example, the lasso is more restrictive than OLS in estimating the coefficients and sets some of them to exactly zero. It is more interpretable than OLS, because in the final model there are less coefficients, so one only has to interpret a subset of the coefficients that would have been included in OLS regression. GAMs extend the linear model by allowing non-linear relationships, but the relationship between <span class="math inline">\(Y\)</span> and <span class="math inline">\(X\)</span> is now modeled as a curve, which complicates interpretation.</p>
<p>Supervised learning: a response <span class="math inline">\(Y\)</span> is related to a set of predictors <span class="math inline">\(X\)</span>, with the aim to predict or infer the relationship between the two.</p>
<p>Unsupervised learning: We only have predictors <span class="math inline">\(X\)</span> and the goal is to understand relations between the variables or between the observations.</p>
<p>Regression problems: quantitative response.</p>
<p>Classification problems: qualitative response.</p>
<p>However, logistic regression is typically used in a qualitative setting, but as it estimates probabilities, it can be thought of as a regression method as well. Some methods (K-nearest neighbors; boosting) can be used for both quantitative or qualitative responses.</p>
</div>
<div id="model-accuracy" class="section level4">
<h4><span class="header-section-number">4.1.1.6</span> Model accuracy</h4>
<p><span class="math display">\[
MSE = 
\frac{1}{n} \sum^n_{i = 1} (y_i - \hat{f}(x_i))^2 = 
\frac{1}{n} \sum^n_{i = 1} (y_i - \hat{y}_i)^2
\]</span></p>
<p>The mean squared error (MSE) should be calculated on the test data. We want to know whether <span class="math inline">\(\hat{f}(x_0)\)</span> is approximately equal to <span class="math inline">\(y_0\)</span>, where <span class="math inline">\((x_0, y_0)\)</span> is a previously unseen test observation not used to train the statistical learning method. Thus, we want the model with the lowest test MSE
<span class="math display">\[
\text{Ave}(y_0-\hat{f}(x_0))^2.
\]</span></p>
<p>The expected test MSE for a given value <span class="math inline">\(x_0\)</span> can be decomposed into the sum of three fundamental quantities: the variance of <span class="math inline">\(\hat{f}(x_0)\)</span>, the squared bias of <span class="math inline">\(\hat{f}(x_0)\)</span> and the variance of the error term <span class="math inline">\(\epsilon\)</span>. To minimize the expected test error, we need to select a method that simultaneously achieves low bias and low variance. Note that the expected test MSE cannot be smaller than <span class="math inline">\(\text{Var}(\epsilon)\)</span>.</p>
<ul>
<li>Variance: the amount by which <span class="math inline">\(\hat{f}\)</span> would change if we estimated it using a different training set.</li>
<li>Bias: the error that is introduced by simplifying a complicated real-life problem with a much simpler model.</li>
</ul>
<p>Generally, more flexible methods induce less bias but more variance. As we increase the flexibility of a class of methods, the bias initially tends to decrease faster than the variance increases, and thus the MSE declines. At some point, relatively little bias can be removed, but the variance increases, inducing a higher expected MSE.</p>
</div>
<div id="residual-sum-of-squares" class="section level4">
<h4><span class="header-section-number">4.1.1.7</span> Residual sum of squares</h4>
<p><span class="math display">\[
\begin{align}
RSS &amp;= e_1^2 + e_2^2 + \dots + e_n^2 \\
    &amp;= (y_1 - \hat{\beta}_0 - \hat{\beta}_1x_1)^2 + \dots + (y_n - \hat{\beta}_0 - \hat{\beta}_1x_n),
\end{align}
\]</span>
with</p>
<p>$$
\begin{aligned}
_1 &amp;=  \</p>
<p>_0 &amp;= {y} - _1{x}.</p>
<p>\end{aligned}
$$</p>
<p>The least squares estimates are unbiased, that is, they do not systematically over or underestimate the true coefficients. We can express the expected amount that the estimates will differ from the truth with the standard error</p>
<p><span class="math display">\[
\text{Var}(\hat{\beta}) = \text{SE}(\hat{\beta})^2 = \frac{\sigma^2}{n},
\]</span>
where <span class="math inline">\(\sigma\)</span> is the standard deviation of each of the realizations <span class="math inline">\(y_i\)</span> of <span class="math inline">\(Y\)</span>. The estimates of the regression coefficients can be computed fairly easily, but for these formulas to be valid, the errors <span class="math inline">\(\epsilon_i\)</span> must be uncorrelated with a common variance <span class="math inline">\(\sigma^2\)</span>.</p>
<p>Note that <span class="math inline">\(\text{SE}(\hat{\beta}_1)\)</span> is smaller when the <span class="math inline">\(x_i\)</span> are more spread out. Intuitively, we have more leverage to estimate a slope when this is the case. In general, <span class="math inline">\(\sigma^2\)</span> is not known, but can be estimated from the data. The estimate of <span class="math inline">\(\sigma\)</span> is known as the residual standard error, and is given by the formula <span class="math inline">\(\sqrt{\frac{\text{RSS}}{(n-p-1)}}\)</span>. Then, we can compute a <span class="math inline">\(95\%\)</span> confidence interval by adding and substracting 2 times the standard error of the coefficients, which indicate that with <span class="math inline">\(95\%\)</span> probability, the CI will contain the true value of the parameter. To test a null hypothesis <span class="math inline">\(H_0: \beta_1 = 0\)</span>, we can compute a t-statistic
<span class="math display">\[
t = \frac{\hat{\beta}_1 - 0}{\text{SE}(\hat{\beta}_1)},
\]</span>
which measures the number of standard deviations <span class="math inline">\(\hat{\beta}_1\)</span> is away from zero. If there is no relationship between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, we expect that <span class="math inline">\(t\)</span> will have a <span class="math inline">\(t\)</span>-distribution with <span class="math inline">\(n-2\)</span> degrees of freedom. Then, we can compute the <span class="math inline">\(p\)</span>-value, which is the probability that we observe any number equal to <span class="math inline">\(|t|\)</span> or larger in absolute value.</p>
</div>
<div id="accuracy-of-the-model" class="section level4">
<h4><span class="header-section-number">4.1.1.8</span> Accuracy of the model</h4>
<p>The quality of a linear regression model is typically assessed using the residual standard error (RSE) and the <span class="math inline">\(R^2\)</span> statistic. The RSE is an estimate of the average amount that the response will deviate from the true regression line, and can be calculated with
<span class="math display">\[
\text{RSE} = 
\sqrt{\frac{1}{n-p-1}\text{RSS}} = 
\sqrt{\frac{1}{n-p-1}\sum^n_{i = 1} (y_i - \hat{y}_i)^2}.
\]</span></p>
<p>The <span class="math inline">\(R^2\)</span> statistic provides an absolute measure of lack of fit of the model to the data
<span class="math display">\[
R^2 = \frac{\text{TSS} - \text{RSS}}{\text{TSS}} = 1 - \frac{\text{RSS}}{TSS},
\]</span>
where
<span class="math display">\[
TSS = \sum^n_{i=1}(y_i - \bar{y})^2.
\]</span></p>
<p>However, determining what a good value of <span class="math inline">\(R^2\)</span> is depends on the data. In the social sciences, a different value can be expected than sometimes in physics, where an <span class="math inline">\(R^2\)</span> that approaches <span class="math inline">\(1\)</span> is not unrealistic.</p>
</div>
<div id="multiple-linear-regression" class="section level4">
<h4><span class="header-section-number">4.1.1.9</span> Multiple linear regression</h4>
<p>We interpret the regression coefficients <span class="math inline">\(\beta_j\)</span> as the average effect on <span class="math inline">\(Y\)</span> of a one-unit increase in <span class="math inline">\(X_j\)</span>, holding all other predictors <span class="math inline">\(X_{j&#39;}\)</span> fixed. To test whether there is a relationship between the response and the predictors, we can compute the <span class="math inline">\(F\)</span>-statistic,
<span class="math display">\[
F = \frac{(\text{TSS} - \text{RSS}) / p}{\text{RSS} / (n - p - 1)}.
\]</span>
If the linear model assumptions are correct, one can show that
<span class="math display">\[
E\Bigg\{\frac{\text{RSS}}{n-p-1}\Bigg\} = \sigma^2,
\]</span>
and that, provided that <span class="math inline">\(H_0\)</span> is true
<span class="math display">\[
E\Bigg\{\frac{\text{TSS} - \text{RSS}}{p}\Bigg\} = \sigma^2.
\]</span>
Hence, when there is no relationship between the response and the predictors, one would expect a value of the <span class="math inline">\(F\)</span>-statistic close to <span class="math inline">\(1\)</span>. If <span class="math inline">\(H_a\)</span> is true, <span class="math inline">\(E\{(\text{TSS} - \text{RSS)/p}\} &gt; \sigma^2\)</span>, so we expect <span class="math inline">\(F &gt; 1\)</span>. To test whether a subset of the regression coefficients are all equal to <span class="math inline">\(0\)</span>, we can test a second model without all predictors of interest (next to the one including all predictors), and test whether the reduction in residual sum of squares of the model containing all parameters is significant. For each individual predictor, the <span class="math inline">\(t\)</span> statistic is exactly equivalent to computing the <span class="math inline">\(F\)</span>-statistic with a model excluding that single parameter. However, the <span class="math inline">\(F\)</span>-statistic accounts for the number of predictors in the model, while all individual <span class="math inline">\(t\)</span>-statistics do not.</p>
</div>
<div id="variable-selection" class="section level4">
<h4><span class="header-section-number">4.1.1.10</span> Variable selection</h4>
<p>Investigating all possible combinations of variables is generally infeasible, because the possible number of combinations is generally extremely large. An alternative can be forward selection - adding the predictor that is most informative (largest reduction in RSS) - or backward selection - remove the variable with the largest p-value. Eventually, we could choose mixed selection - we add the most informative predictors, and remove the predictors with a p-value above a certain threshold in the meantime.</p>
</div>
<div id="model-fit" class="section level4">
<h4><span class="header-section-number">4.1.1.11</span> Model fit</h4>
<p><span class="math inline">\(R^2\)</span> will always increase when adding new predictors. However, the residual standard error (RSE) can increase if new predictors to the model <span class="math inline">\(\text{RSE} = \sqrt{\frac{1}{n - p -1}\text{RSS}}\)</span>, when the decrease in RSS is small relative to the increase in <span class="math inline">\(p\)</span>.</p>
</div>
<div id="predictions---sources-of-uncertainty" class="section level4">
<h4><span class="header-section-number">4.1.1.12</span> Predictions - sources of uncertainty</h4>
<ul>
<li>The estimated regression coefficients do not necessarily represent the truth (i.e., reducible error).</li>
<li>A linear model is at best an approximation of the truth (i.e., also reducible error called model bias).</li>
<li>Random error - irreducible error, can be taken into account with a prediction interval.</li>
</ul>
</div>
<div id="assumptions-of-additivity-and-linearity" class="section level4">
<h4><span class="header-section-number">4.1.1.13</span> Assumptions of additivity and linearity</h4>
<ul>
<li>Additivity: the effect of changes in <span class="math inline">\(X_j\)</span> on <span class="math inline">\(Y\)</span> is independent from the values of <span class="math inline">\(X_{j&#39;}\)</span>. However, this assumption can be relaxed / overcome by including interaction terms, so that the effect of <span class="math inline">\(X_j\)</span> on <span class="math inline">\(Y\)</span> is no longer constant.</li>
<li>Linearity: the change in <span class="math inline">\(Y\)</span> due to a one-unit increase in <span class="math inline">\(X_j\)</span> is constant, regardless of the value of <span class="math inline">\(X_j\)</span>. Can be relaxed by using polynomial regression.</li>
</ul>
</div>
<div id="potential-problems" class="section level4">
<h4><span class="header-section-number">4.1.1.14</span> Potential problems</h4>
<ul>
<li><strong>Non-linearity</strong>: inspect using residual plots, problems can be overcome by using non-linear transformations of the predictors.</li>
<li><strong>Correlated error terms</strong>: results in underestimated standard errors. Correlated error terms may show up as tracking of the residuals.</li>
<li><strong>Non-constant variance of error terms</strong>: possibly, <span class="math inline">\(Y\)</span> can be transformed to overcome this problem (<span class="math inline">\(\text{log}Y\)</span> or <span class="math inline">\(\sqrt{Y}\)</span>).</li>
<li><strong>Outliers</strong>: when outliers are due to coding errors, they may be removed. However, outliers may also be due to model misspecification.</li>
<li><strong>High leverage points</strong>: have an unusual value on <span class="math inline">\(x_i\)</span> and tend to influence the estimated coefficients. They may be indicated by the leverage statistics.</li>
<li><strong>Collinearity</strong>: can make it difficult to separate out the effects of collinear variables on the response.</li>
<li><strong>Multicollinearity</strong>: a high correlation between three or more variables, even if no pairs of observations have a particularly high correlation. Can be assessed using VIF.</li>
</ul>
</div>
<div id="summary" class="section level4">
<h4><span class="header-section-number">4.1.1.15</span> Summary</h4>
<p>Parametric methods are easy to fit, because one only needs to estimate a relatively small number of coefficients, and if linear regression is used, coefficients have a simple interpretation. However, by definition, parametric models make strong assumptions about the functional form of <span class="math inline">\(f(X)\)</span>. If the specified functional form is far from the truth, and predictive accuracy is our goal, the parametric method will perform poorly. Non-parametric methods do not explicitly assume a parametric form for <span class="math inline">\(f(X)\)</span>, and thereby provide more flexibility.</p>
<p>K-nearest neighbor regression - given a value for <span class="math inline">\(K\)</span> and a prediction point <span class="math inline">\(x_0\)</span>, KNN regression identifies the <span class="math inline">\(K\)</span> training observations closest to <span class="math inline">\(x_0\)</span> (denoted <span class="math inline">\(\mathcal{N}_0\)</span>) and estimates <span class="math inline">\(f(x_0)\)</span> using the average of all training reponses in <span class="math inline">\(\mathcal{N}_0\)</span>. The optimal value for <span class="math inline">\(K\)</span> depends on the bias / variance trade-off.</p>
<p>Parametric approaches will outperform non-parametric approaches if the selected parametric form is close to the true form of <span class="math inline">\(f\)</span>. Also, in higher dimensions, KNN often performs worse than linear regression, due to the curse of dimensionality (the nearest observations to a given test observation <span class="math inline">\(x_0\)</span> may be far away from <span class="math inline">\(x_0\)</span> in <span class="math inline">\(p\)</span>-dimensional space, especially when <span class="math inline">\(p\)</span> is large), leading to poor predictions.</p>
</div>
</div>
<div id="chapter-6---model-selection" class="section level3">
<h3><span class="header-section-number">4.1.2</span> Chapter 6 - Model selection</h3>
<div id="prediction-accuracy" class="section level4">
<h4><span class="header-section-number">4.1.2.1</span> Prediction accuracy</h4>
<p>If the true relationships are approximately linear, least squares estimates generally have low bias, and if <span class="math inline">\(n\)</span> is much larger than <span class="math inline">\(p\)</span>, least squares estimates also have low variance. If <span class="math inline">\(n\)</span> is not much larger than <span class="math inline">\(p\)</span>, there can be a lot of variability in the least squares fit, resulting in overfitting and poor predictions on test data. If <span class="math inline">\(p &gt; n\)</span>, there is no unique least squares coefficient estimate: the variance is infinite. By constraining or shrinking the estimated coefficients, we can often substantially reduce the variance, while only slightly increasing the variance. By removing irrelevant variables, the model interpretability increases, as we have to look at less estimated coefficients.</p>
</div>
<div id="subset-selection" class="section level4">
<h4><span class="header-section-number">4.1.2.2</span> Subset selection</h4>
<p><strong>Best subset selection</strong>: fit a regression model on all <span class="math inline">\(2^p\)</span> possible models, and use the best one. Usually, best subset selection is broken up into two stages:</p>
<ol style="list-style-type: decimal">
<li><p>Let <span class="math inline">\(M_0\)</span> denote the null / intercept-only model, and for <span class="math inline">\(k = 1, 2, \dots, p\)</span>, do:</p>
<p>1.1 Fit all <span class="math inline">\(p \choose k\)</span> models that contain exactly <span class="math inline">\(k\)</span> predictors.</p>
<p>1.2 Pick the best model (smallest RSS) and call it <span class="math inline">\(M_k\)</span>.</p></li>
<li><p>Select a single best model from among <span class="math inline">\(M_0, \dots, M_p\)</span> using the cross-validated prediction error, <span class="math inline">\(C_p~(AIC)\)</span>, <span class="math inline">\(BIC\)</span> or <span class="math inline">\(\text{adjusted}~R^2\)</span>.</p></li>
</ol>
<p>Step 1 identifies the best model on the training data for each subset size, the the reduced number of models considered in cross-validation. When a logistic regression model is of interest, we can also perform cross-validation, but we then look at the deviance instead of RSS / <span class="math inline">\(R^2\)</span>. As <span class="math inline">\(p\)</span> gets very large (<span class="math inline">\(&gt; 40\)</span>), best subset selection is infeasible, even with extremely fast computers.</p>
</div>
<div id="stepwise-selection" class="section level4">
<h4><span class="header-section-number">4.1.2.3</span> Stepwise selection</h4>
<p><strong>Forward stepwise selection</strong></p>
<ol style="list-style-type: decimal">
<li><p>Let <span class="math inline">\(M_0\)</span> denote the null model, and for <span class="math inline">\(k = 0, \dots, p-1\)</span></p>
<p>1.1 Consider all <span class="math inline">\(p-k\)</span> models that augment the predictors in <span class="math inline">\(M_k\)</span> with one additional predictor.</p>
<p>1.2 Choose the best among these <span class="math inline">\(p - k\)</span> models and call it <span class="math inline">\(M_{k+1}\)</span>, where best is defined as having the lowest RSS / highest R^2.</p></li>
<li><p>Select a signle best model from among <span class="math inline">\(M_0, \dots, M_p\)</span> using the cross-validated prediction error, <span class="math inline">\(C_p~(AIC)\)</span>, <span class="math inline">\(BIC\)</span> or <span class="math inline">\(\text{adjusted}~R^2\)</span>.</p></li>
</ol>
<p>Instead of <span class="math inline">\(2^p\)</span> models, we now only consider the null model and the <span class="math inline">\(p-k\)</span> models in the <span class="math inline">\(k^{th}\)</span> iteration, in total <span class="math inline">\(1 + p(p+1)/2\)</span>. Although forward selection tends to do well in practice, it is not guaranteed to find the best possible model, as not all combinations are considered. Forward selection can also be applied when <span class="math inline">\(n &lt; p\)</span>, but then models with <span class="math inline">\(p &gt; n\)</span> are not considered.</p>
<p><strong>Backward stepwise selection</strong></p>
<ol style="list-style-type: decimal">
<li><p>Let <span class="math inline">\(M_p\)</span> denote the full model containing <span class="math inline">\(p\)</span> predictors and for <span class="math inline">\(k = p, p-1, \dots, 1\)</span>, do:</p>
<p>1.1 Consider all <span class="math inline">\(k\)</span> models that contain all but one of the predictors in <span class="math inline">\(M_k\)</span>, for a total of <span class="math inline">\(k - 1\)</span> predictors.</p>
<p>1.2 Choose the best among these <span class="math inline">\(k\)</span> models and call it <span class="math inline">\(M_{k-1}\)</span> (smallest RSS / largest <span class="math inline">\(R^2\)</span>).</p></li>
<li><p>Select a signle best model from among <span class="math inline">\(M_0, \dots, M_p\)</span> using the cross-validated prediction error, <span class="math inline">\(C_p~(AIC)\)</span>, <span class="math inline">\(BIC\)</span>, or <span class="math inline">\(\text{adjusted}~R^2\)</span>.</p></li>
</ol>
<p>Backward selection is not guaranteed to find the best model containing a subset of the <span class="math inline">\(p\)</span> predictors, and it is required that <span class="math inline">\(n &gt; p\)</span>.</p>
<p><strong>Hybrid approaches</strong></p>
<p>Variables are added sequantially, as in forward selection, but after any addition, variables that no longer contribute to the fit may be removed.</p>
</div>
<div id="choosing-the-optimal-model" class="section level4">
<h4><span class="header-section-number">4.1.2.4</span> Choosing the optimal model</h4>
<p>Choose a model with the lowest test error. The training error can be a poor estimate of the test error, and therefore, RSS and <span class="math inline">\(R^2\)</span> are not appropriate for selecting the best model among a collection of models with different numbers of predictors.</p>
<p>There are generally two approaches to estimate the test error:</p>
<ol style="list-style-type: decimal">
<li><p>Indirectly estimate the test error by making an adjustment to the training error to account for the bias due to overfitting.</p></li>
<li><p>Directly estimate the test error by using a validation set / cross-validation approach.</p></li>
</ol>
<p>The MSE <span class="math inline">\((RSS / n)\)</span> in the training set is generally an underestimate of the test MSE. The training error will namely decrease as we add predictors to the model, while the test error may not. Therefore, we need methods that adjust for the number of predictors.</p>
<p><span class="math display">\[
C_p = \frac{1}{n}(RSS + 2d\hat{\sigma}^2),
\]</span>
with <span class="math inline">\(d\)</span> the number of predictors and <span class="math inline">\(\hat{\sigma}^2\)</span> an estimate of the residual error variance. Typically, <span class="math inline">\(\hat{\sigma}^2\)</span> is estimated using the full model containing all predictors. Thus, the <span class="math inline">\(C_p\)</span> statistic adds a penalty of <span class="math inline">\(2d\hat{\sigma}^2\)</span> to the training RSS to adjust for the fact that the training error tends to underestimate the test error. If <span class="math inline">\(\hat{\sigma}^2\)</span> is an unbiased estimate of <span class="math inline">\(\sigma^2\)</span>, <span class="math inline">\(C_p\)</span> is an unbiased estimate of the test MSE.
<span class="math display">\[
AIC = \frac{1}{n\hat{\sigma}^2}(RSS + 2d\hat{\sigma}^2),
\]</span>
and for least squares models, <span class="math inline">\(C_p\)</span> and <span class="math inline">\(AIC\)</span> are proportional to each other.
<span class="math display">\[
BIC = \frac{1}{n\hat{\sigma}^2}(RSS + log(n)d\hat{\sigma}^2)
\]</span>
Like <span class="math inline">\(C_p\)</span>, the <span class="math inline">\(BIC\)</span> takes on a smaller value for better models. However, the <span class="math inline">\(BIC\)</span> replaces <span class="math inline">\(2d\hat{\sigma}^2\)</span> used by <span class="math inline">\(C_p\)</span> with <span class="math inline">\(log(n)d\hat{\sigma}^2\)</span>, where <span class="math inline">\(n\)</span> is the number of observations included. Since <span class="math inline">\(log(n) &gt; 2\)</span> for any <span class="math inline">\(n &gt; 7\)</span>, the <span class="math inline">\(BIC\)</span> is generally more restrictive for models with more parameters, and thus tends to select smaller models than the <span class="math inline">\(C_p\)</span> / <span class="math inline">\(AIC\)</span>.</p>
<p><span class="math display">\[
\text{Adjusted}~R^2 = 1 - \frac{RSS/(n-d-1)}{TSS/(n-1)}
\]</span>
Unlike <span class="math inline">\(C_p\)</span>, <span class="math inline">\(AIC\)</span> and <span class="math inline">\(BIC\)</span>, a large value of adjusted <span class="math inline">\(R^2\)</span> is indicative for a model with a low test error. So, maximizing the adjusted <span class="math inline">\(R^2\)</span> is equivalent to minimizing <span class="math inline">\(\frac{RSS}{n-d-1}\)</span>, and thus the <span class="math inline">\(\text{adjusted}~R^2\)</span> may increase or decrease, due to taking the number of parameters <span class="math inline">\(d\)</span> into account.</p>
</div>
<div id="validation-and-cross-validation" class="section level4">
<h4><span class="header-section-number">4.1.2.5</span> Validation and cross-validation</h4>
<p>We can also estimate the test error directly, using (cross-) validation methods. We can compute the validation set error or the cross-validation error for each model under consideration, and select the model for which the resulting estimated test error is smallest.</p>
<p>Contrary to the <span class="math inline">\(C_p\)</span>, <span class="math inline">\(AIC\)</span>, <span class="math inline">\(BIC\)</span> and <span class="math inline">\(\text{adjusted}~R^2\)</span>, it provides a direct estimate of the test error and makes fewer assumptions about the true underlying model. It can also be used in a wider range of model selection tasks, for example when it is hard to estimate the error variance <span class="math inline">\(\sigma^2\)</span> or pinpoint the model degrees of freedom.</p>
</div>
</div>
<div id="chapter-7---shrinkage-methods" class="section level3">
<h3><span class="header-section-number">4.1.3</span> Chapter 7 - Shrinkage methods</h3>
<p>As an alternative to subset selection methods, we can fit a model containing all <span class="math inline">\(p\)</span> predictors using a technique that constrains or regularizes the coefficient estimates, or, equivalently, that shrinks the coefficient estimates toward zero.</p>
<div id="ridge-regression" class="section level4">
<h4><span class="header-section-number">4.1.3.1</span> Ridge regression</h4>
<p>The least squares fitting procedure estimates <span class="math inline">\(\beta_0, \beta_1, \dots, \beta_p\)</span>, using the values that minimize
<span class="math display">\[
RSS = \sum^n_{i=1}(y_i - \beta_0 - \sum^p_{j=1}\beta_jx_{ij})^2.
\]</span>
Ridge regression is very similar to least squares, except that the coefficients are estimated by minimizing a slightly different quantity
<span class="math display">\[
\sum^n_{i=1}(y_i - \beta_0 - \sum^p_{j=1}\beta_jx_{ij})^2 + 
\lambda\sum^p_{j=1}\beta^2_j = RSS + \lambda\sum^p_{j=1}\beta^2_j
\]</span></p>
<p>where <span class="math inline">\(\lambda \geq 0\)</span> is a tuning parameter, to be determined separately. As with least squares, ridge regression seeks coefficient estimates that fit the data well, by making the RSS small. The second term, <span class="math inline">\(\lambda\sum^p_{j=1}\beta^2_j\)</span>, called a shrinkage penalty is small when <span class="math inline">\(\beta_1, \dots, \beta_p\)</span> are close to zero, and so it has the effect of shrinking the estimates of <span class="math inline">\(\beta_j\)</span> towards zero. The tuning parameter <span class="math inline">\(\lambda\)</span> serves to control the relative impact of this term on the regression coefficient estimates. When <span class="math inline">\(\lambda = 0\)</span>, the penalty term has no effect and ridge regression will produce the least squares estimates. Ridge regression will produce a different set of coefficient estimates, <span class="math inline">\(\hat{\beta}^R_{\lambda}\)</span>, for each value of <span class="math inline">\(\lambda\)</span>. Note that we do not shrink the value of the intercept, as it remains the value of <span class="math inline">\(Y\)</span> when all <span class="math inline">\(X&#39;s\)</span> are zero.</p>
<p>Standard least squares regression coefficient estimates are scale equivariant; regardless of the scale of <span class="math inline">\(X_j\)</span>, <span class="math inline">\(X_j\hat{\beta}_j\)</span> will remain the same. The ridge regression coefficients can change substantially when the scale is changed. Namely, <span class="math inline">\(X_j\hat{\beta}^R_{j,\lambda}\)</span> will not only depend on the value of <span class="math inline">\(\lambda\)</span>, but also on the scaling of the other predictors and on the scaling of the <span class="math inline">\(j^{th}\)</span> predictor. Therefore, it is best to apply ridge regression after standardizing the predictors.</p>
<p>Ridge regressions advantage over least squares is rooted in the bias-variance trade-off. As <span class="math inline">\(\lambda\)</span> increases, the flexibility of the ridge regression fit decreases, leading to decreased variance but increased bias. When the relationship between the response and the predictors is linear, least squares estimates will have low bias, but may have a high variance. When <span class="math inline">\(p\)</span> is almost as large as <span class="math inline">\(n\)</span>, least squares coefficients will be extremely variable, and if <span class="math inline">\(p &gt; n\)</span>, least squares estimates do not have a unique solution. In these situations, ridge regression can still perform well by trading off a small increase in bias for a large decrease in variance. Hence, ridge regression works best when least squares estimates have high variance. Also, ridge regression is computationally much more efficient than best subset selection.</p>
</div>
<div id="the-lasso" class="section level4">
<h4><span class="header-section-number">4.1.3.2</span> The lasso</h4>
<p>Ridge regression has one obvious disadvantage, as it will include all <span class="math inline">\(p\)</span> predictors in the final model. The penalty <span class="math inline">\(\lambda \sum^p_{j=1} \hat{\beta}_j^2\)</span> will shrink all coefficients towards zero, but it will not set any of them to exactly zero (unless <span class="math inline">\(\lambda = \infty\)</span>), which may complicate model interpretation (especially when <span class="math inline">\(p\)</span> is large). The lasso overcomes this disadvantage, by estimating coefficients that minimize the quantity</p>
<p><span class="math display">\[
\sum^n_{i=1}(y_i - \beta_0 - \sum^p_{j=1}\beta_jx_{ij})^2 + \sum^p_{j=1}|\beta_j|
= RSS + \lambda\sum^p_{j=1}|\beta_j|.
\]</span></p>
<p>So, the term <span class="math inline">\(\beta^2_j\)</span> in the ridge regression penalty is replaced by <span class="math inline">\(|\beta_j|\)</span>. This penalty has the effect of forcing some of the coefficient estimates to be exactly equal to zero when <span class="math inline">\(\lambda\)</span> is sufficiently large. So, the lasso performs variable selection, and thus the lasso yields sparse models that are easier to interpret.</p>
<p>When <span class="math inline">\(\lambda = 0\)</span>, the lasso provides the least squares fit, and when <span class="math inline">\(\lambda = \infty\)</span>, all coefficients are set equal to 0, just as ridge regression. However, in between these two extremes, the ridge regression and lasso models are quite different. Depending on <span class="math inline">\(\lambda\)</span>, the lasso can produce a model involving any number of predictors, while ridge regression will always include all variables in the model.</p>
<p>When we perform the lasso, we are trying to find a set of coefficient estimates that lead to the smallest RSS, subject to the constraint that there is a budget <span class="math inline">\(s\)</span> for how large <span class="math inline">\(\sum^p_{j=1}|\beta_j|\)</span> can be. When <span class="math inline">\(s\)</span> is extremely large, the budget is not very restrictive, and so the parameters can be large. In fact, if <span class="math inline">\(s\)</span> is large enough that the least squares solution falls within the budget, the lasso will give the least squares solution. But if <span class="math inline">\(s\)</span> is small, then <span class="math inline">\(\sum^p_{j=1}|\beta_j|\)</span> must be small in order to avoid violating the budget. Similarly, when we perform ridge regression, we seek a set of coefficient estimates such that the RSS is as small as possible subject to the requirement that <span class="math inline">\(\sum^p_{j=1}\beta^2_j\)</span> not exceeds the budget <span class="math inline">\(s\)</span>.</p>
<p>We can interpret ridge regression and the lasso as computationally feasible alternatives to best subset selection (one can regard the budget as the number of variables that must be set equal to 0). Then, the lasso is closest to best subset selection, since only the lasso performs subset selection, given that <span class="math inline">\(s\)</span> is sufficiently small.</p>
<p>Since ridge regression has a circular constraint with no sharp point, the intersection of <span class="math inline">\(s\)</span> and the RSS will not generally occur on an axis, and so the regression coefficient estimates will be exclusively non-zero. The lasso constraints have corners at each of the aces, and so the ellipses will often intersect the constraint region at an axis. If all predictors in a dataset/model are truely related to the response, ridge regression will generally outperform the lasso in terms of prediction error. The lasso namely implicitly assumes that a number of the coefficients are truly equal to zero. Neither ridge regression not the lasso will universally dominate the other. In general, one might expect the lasso to perform better when a relatively small number of predictors have substantial effects, while the remaining predictors have small or no effects. Ridge regression will perform better when many predictors all have similar effects. However, in practice the relationships between predictors and response is not known a priori. Techniques as cross-validation can be used in order to determine which approach is better on a particular data set.</p>
<p>As with ridge regression, when the least squares estimates have excessively high variance, the lasso solution can yield a reduction in variance at the expense of a small increase in bias. Ridge regression more or less shrinks every dimension of the data by the same proportion, whereas the lasso more or less shrinks all coefficients toward zero by a similar amount (soft-thresholding), and sufficiently small coefficients are shrunken all the way to zero.</p>
</div>
<div id="selecting-the-tuning-parameter" class="section level4">
<h4><span class="header-section-number">4.1.3.3</span> Selecting the tuning parameter</h4>
<p>We choose a grid of <span class="math inline">\(\lambda\)</span> values, and compute the cross-validation error for each value of <span class="math inline">\(\lambda\)</span>. We select the <span class="math inline">\(\lambda\)</span> value for which the cross-validation error is smallest. Finally, the model is refit using all of the available observations, and the selected value of the tuning parameter.</p>
</div>
</div>
<div id="resampling-methods" class="section level3">
<h3><span class="header-section-number">4.1.4</span> Resampling Methods</h3>
<ul>
<li>Model assessment - the process of evaluating a models performance.</li>
<li>Model selection - the process of selecting the proper level of flexibility for a model.
In the absence of a very large designated test set that can be used to directly estimate the test error rate, a number of techniques can be used to estimate this quantity using the available training data.</li>
</ul>
<div id="the-validation-set-approach" class="section level4">
<h4><span class="header-section-number">4.1.4.1</span> The validation set approach</h4>
<p>One randomly divides the available data into a training set and a validation set. The model is than fitted / trained on the training set, and the fitted model is used to predict the responses for the observations in the validation set. The resulting validation set error rate (typically assessed by the MSE) provides an estimate of the test error rate.</p>
<ul>
<li>The validation estimate of the test error rate can be highly variable, depending on precisely which observations are included in the training set and which observations are included in the validation set.</li>
<li>In the validation approach, only a subset of the observations - those in the training set - are used to fit the model. Since statistical methods tend to perform worse when trained on fewer observations, the validation set error rate might overestimate the test error rate for the model fit on the entire dataset.</li>
</ul>
</div>
<div id="leave-one-out-cross-validation" class="section level4">
<h4><span class="header-section-number">4.1.4.2</span> Leave-one-out cross-validation</h4>
<p>LOOCV uses a single observation as the validation set. This procedure is repeated for all <span class="math inline">\(n\)</span> observations:</p>
<p><span class="math display">\[
CV_n = \frac{1}{n}\sum^n_{i=1}MSE_i.
\]</span>
Advantages of LOOCV over the validation set approach are that LOOCV has far less bias, as much more of the information in the data is used to train the model. Consequently, LOOCV tends not to oberestimate the test error rate as much as the validation set approach does. Also, in contrast to the validation set approach, which will yield different results when applied repeatedly due to randomness in training/validation splits, LOOCV will always yield the same result, as there is no randomness.</p>
<p>However, LOOCV has the potential to be expensive to implement in terms of time, especially when <span class="math inline">\(n\)</span> is large or when each individual model is slow to fit. However, with least squares linear or polynomial regression, a shortcut makes the cost of LOOCV the same as that of a single model fit.</p>
<p><span class="math display">\[
CV_{(n)} = \frac{1}{n} \sum^n_{i=1}\Bigg(\frac{y_i-\hat{y}_i}{1-h_i}\Bigg)^2
\]</span>
where <span class="math inline">\(\hat{y}_i\)</span> is the <span class="math inline">\(i^{th}\)</span> fitted value, and <span class="math inline">\(h_i\)</span> is the leverage of this observation. This is like the ordinary MSE, except the <span class="math inline">\(i^{th}\)</span> residual is divided by <span class="math inline">\(1-h_i\)</span>. The leverage lies between <span class="math inline">\(\frac{1}{n}\)</span> and <span class="math inline">\(n\)</span> and reflects the amount that an observation influences its own fit. Hence, the residuals for high-leverage points are inflated in this formula by exactly the right amount for this equality to hold.</p>
</div>
<div id="k-fold-cross-validation" class="section level4">
<h4><span class="header-section-number">4.1.4.3</span> k-Fold cross-validation</h4>
<p>Randomly dividing the set of observations into <span class="math inline">\(k\)</span> groups or folds of approximately the same size. For <span class="math inline">\(i = 1, \dots, k\)</span> the model is fit on data <span class="math inline">\(k&#39;\)</span> and validated on <span class="math inline">\(k\)</span> using the MSE, so there are <span class="math inline">\(k\)</span> estimates of the test error: <span class="math inline">\(MSE_1, MSE_2, \dots, MSE_k\)</span>.</p>
<p><span class="math display">\[
CV_k = \frac{1}{k} \sum^k_{i=1}MSE_i
\]</span></p>
<p>LOOCV is a special case of <span class="math inline">\(k\)</span>-fold cross-validation with <span class="math inline">\(k=n\)</span>. Typically, <span class="math inline">\(k\)</span>-fold cross-validation is performed with <span class="math inline">\(k=5\)</span> or <span class="math inline">\(k=10\)</span>. The main advantage of <span class="math inline">\(k\)</span>-fold CV is computational. Also, <span class="math inline">\(k\)</span>-fold CV is a very general approach that can be applied to almost any statistical learning method. Some statistical learning methods have computationally intensive fitting procedures, and performing LOOCV may pose computational problems, especially if <span class="math inline">\(n\)</span> is extremely large. 10-fold CV might ten be much more feasible.</p>
<p>When using 10-fold CV, there is some variability in the CV estimates, as a result of the variability in how the observations are divided into folds, but this variability is typically much lower than the variability in the test error estimates that result from the validation set approach.</p>
<p>When we perform cross-validation, our goal might be to determine how well a given statistical learning procedure can be expected to perform on independent data. In this case, the actual estimate of the test MSE is of interest. Other times, we are interested only in the location of the minimum point in the estimated test MSE curve, because we might be performing CV on a number of statistical learning methods, or on a single method using different levels of flexibility in order to identify the method that results in the lowest test error. Then, the actual value of the estimated test MSE is not very important. Despite the fact that LOOCV and <span class="math inline">\(k\)</span>-fold CV sometimes underestimate the true test MSE, both approaches come close to identifying the correct level of flexibility.</p>
<p><span class="math inline">\(k\)</span>-Fold cross validation often gives more accurate estimates of the test error rate than LOOCV, due to the bias-variance trade-off. The validation set approach can lead to overestimates of the test error rate, since it uses only part of the information in the data. Extending this viewpoint, <span class="math inline">\(k\)</span>-fold CV should perform somewhat better, and LOOCV is to be preferred over <span class="math inline">\(k\)</span>-fold CV.</p>
<p>However, it turns out that LOOCV has higher variance than <span class="math inline">\(k\)</span>-fold cross-validation with <span class="math inline">\(k&lt;n\)</span>. When we perform LOOCV, we are averaging the outputs of <span class="math inline">\(n\)</span> fitted models, each of which is trained on a nearly identical set of observations. Therefore, the outputs are highly positively correlated with each other. With <span class="math inline">\(k\)</span>-fold CV with <span class="math inline">\(k &lt; n\)</span> the correlations are smaller, as the overlap between the training sets is smaller. Since the mean of many highly correlated quantities has higher variance than the mean of many quantities that are not as highly correlated, the test error estimate resulting from LOOCV tends to have higher variance than the test error estimate resulting from <span class="math inline">\(k\)</span>-fold CV.</p>
</div>
<div id="the-bootstrap" class="section level4">
<h4><span class="header-section-number">4.1.4.4</span> The bootstrap</h4>
<p>The bootstrap allows to use a computer to emulate the process of obtaining new sample sets, so that we can estimate the variability of <span class="math inline">\(X\)</span> without generating additional samples. That is, we obtain distinct datasets by repeatedly sampling observations from the original data. We can compute the standard error of the bootstrap estimates using the formula
<span class="math display">\[
SE_B(X) = \sqrt{\frac{1}{B-1}\sum^B_{r=1}X^{*r}-\frac{1}{B}\sum^B_{r&#39;=1}X^{*r&#39;}}.
\]</span>
This serves as an estimate of the standard error of <span class="math inline">\(X\)</span> estimated from the original dataset.</p>
</div>
</div>
<div id="moving-beyond-linearity" class="section level3">
<h3><span class="header-section-number">4.1.5</span> Moving beyond linearity</h3>
<div id="polynomial-regression" class="section level4">
<h4><span class="header-section-number">4.1.5.1</span> Polynomial regression</h4>
<p>Adding extra predictors obtained by raising each of the original predictors to a power
<span class="math display">\[
y_i = \beta_0 + \beta_1x_i + \beta_2x_i^2 + \dots + \beta_d x_i^2 + \epsilon_i,
\]</span>
can be estimated using least squares. Generally, it is unusual to use <span class="math inline">\(d\)</span> greater than 3 or 4, because for large values of <span class="math inline">\(d\)</span>, the polynomial curve can become overly flexible and can take on some very strange shapes, especially near boundary values of <span class="math inline">\(X\)</span>.</p>
</div>
<div id="step-functions" class="section level4">
<h4><span class="header-section-number">4.1.5.2</span> Step functions</h4>
<p>Polynomial functions impose a global structure on the non-linear function of <span class="math inline">\(X\)</span>. We can use step functions to avoid imposing such a global structure. We break the range of <span class="math inline">\(X\)</span> into bins, and fit a different constant in each bin. This amounts to converting a continuous variable into an ordered categorical variable, by using dummies (i.e., an indicator variable). Then, the model can be written as
<span class="math display">\[
y_i = \beta_0 + \beta_1 C_1(x_i) + \beta_2 C_2(x_i) + \dots + \beta_K C_K(x_i) + \epsilon_i
\]</span>
where <span class="math inline">\(C_1, C_2, \dots, C_K\)</span> are the indicator functions that return a <span class="math inline">\(1\)</span> if observation <span class="math inline">\(i\)</span> belongs to that range of <span class="math inline">\(X\)</span> values, and a zero otherwise. For any given <span class="math inline">\(X\)</span>, at most one of the <span class="math inline">\(C_k\)</span> can be non-zero. When <span class="math inline">\(X &lt; c_1\)</span>, all of the predictors are zero, and <span class="math inline">\(\beta_0\)</span> represents the mean of <span class="math inline">\(Y\)</span> for <span class="math inline">\(X &lt; C_1\)</span>. Unfortunately, unless there are natural breakpoints, piecewise-constant functions can miss important trends in the data.</p>
</div>
<div id="basis-functions" class="section level4">
<h4><span class="header-section-number">4.1.5.3</span> Basis functions</h4>
<p>Polynomial and piecewise-constant functions are in fact special cases of a basis function approach: we use a family of functions or transformations applied to a variable <span class="math inline">\(X\)</span>: <span class="math inline">\(b_1(X), b_2(X), \dots, b_K(X)\)</span>. Thus, we fit the model
<span class="math display">\[
y_i = \beta_0 + \beta_1 b_1(x_i) + \beta_2 b_2(x_i) + \dots + \beta_K b_K(x_i) + \epsilon_i.
\]</span></p>
<p>The basis functions are fixed and known, i.e., they are specified before fitting the model. We can see the basis functions as the predictors in a standard linear model, and hence we can use least squares to estimate the unknown coefficients. Thus, all inferences tools of linear regression (SEs, <span class="math inline">\(R^2\)</span>, <span class="math inline">\(F\)</span>-tests) are readily available.</p>
</div>
<div id="regression-splines" class="section level4">
<h4><span class="header-section-number">4.1.5.4</span> Regression splines</h4>
<p>Piecewise polynomial regression involves fitting a separate low degree polynomial over different regions of X. The points where the coefficients change are called the knots. A piecewise cubic polynomial with a single knot at a point <span class="math inline">\(c\)</span> takes the form
<span class="math display">\[
y_i =
  \begin{cases}
    \beta_{01} + \beta_{11}x_i + \beta_{21}x^2_i + \beta_{31}x^3_i + \epsilon_i   ~~~~~\text{if}~x_i &lt; c_i \\
    \beta_{02} + \beta_{12}x_i + \beta_{22}x^2_i + \beta_{32}x^3_i + \epsilon_i   ~~~~~\text{if}~x_i \geq c_i. \\
  \end{cases}
\]</span>
Each of these polynomial functions can be fit using least squares applied to simple functions of the original predictor.</p>
<p>Using more knots leads to a more flexible piecewise polynomial. Piecewise constant functions are piecewise polynomials of degree zero. However, a problem of piecewise polynomial regression is that it is likely to be a discontinuous function. A piecewise polynomial function of the <span class="math inline">\(j^{th}\)</span> power with <span class="math inline">\(k\)</span> knots uses <span class="math inline">\(k(j+1)\)</span> degrees of freedom.</p>
<p>However, we can fit a piecewise polynomial under the constraint that the fitted curve must be continuous. However, there may then be sharp angles in the fitted regression slopes that look unnatural as well. We can remedy this by adding the constraints that the first and second derivative must be the same. Then, it is required that the transition at the knot is also very smooth. Each constraint that we impose effectively frees one degree of freedom. When we also constrain the first and second derivative to be equal, we end up with a cubic spline. In general, a cubic spline with <span class="math inline">\(K\)</span> knots uses <span class="math inline">\(4 + K\)</span> degrees of freedom. The general definition of a degree-<span class="math inline">\(d\)</span> spline is that it is a piecewise degree-<span class="math inline">\(d\)</span> polynomial, with continuity in derivatives up to degree <span class="math inline">\(d-1\)</span> at each knot.</p>
</div>
<div id="the-splines-basis-representation" class="section level4">
<h4><span class="header-section-number">4.1.5.5</span> The splines basis representation</h4>
<p>We can use the basis model to represent a regression spline. A cubic spline with <span class="math inline">\(K\)</span> knots can be represented as
<span class="math display">\[
y_i = \beta_0 + \beta_1 b_1(x_i) + \beta_2 b_2(x_i) + \dots + \beta_{K+3} b_{K+3}(x_i) + \epsilon_i
\]</span>
for an appropriate choice of basis functions <span class="math inline">\(b_1, b_2, \dots, b_{K+3}\)</span>. The model can be fit using least squares. The most direct way to represent a cubic spline is to start off with a basis for a cubic polynomial - namely, <span class="math inline">\(x, x^2, x^3\)</span> - and then add one truncated power basis function per knot. A truncated power basis function is defined as
<span class="math display">\[
h(x,\xi)^3_+ = 
  \begin{cases}
  (x-\xi) &amp;\text{if}~ x&gt; \xi \\
  0 &amp;\text{otherwise},
  \end{cases}
\]</span>
where <span class="math inline">\(\xi\)</span> is the knot. Adding the term of the form <span class="math inline">\(\beta_4h(x,\xi)\)</span> for a cubic polynomial will lead to a discontinuity in only the third derivative at <span class="math inline">\(\xi\)</span>; the function will remain continuous, with continuous first and second derivatives at each of the knots.</p>
<p>In order to fit a cubic spline to a dataset with <span class="math inline">\(K\)</span> knots, we perform least squares regression with an intercept and <span class="math inline">\(3+K\)</span> predictors of the form <span class="math inline">\(X, X^2, X^3, h(X, \xi_1), h(X,\xi_2), \dots, h(X,\xi_K)\)</span>, where <span class="math inline">\(\xi_1, \dots, \xi_K\)</span> are the knots. This amounts to estimating a total of <span class="math inline">\(K+4\)</span> degrees of freedom. Unfortunately, splines can have high variance at the outer range of the predictors (i.e., where <span class="math inline">\(X\)</span> is small or large). To overcome this, one can use a natural spline, which is a regression spline with additional boundary constraints: the function is required to be linear at the boundary (in the region where <span class="math inline">\(X\)</span> is smaller (larger) than the smallest (largest) knot). This means that natural splines generally produce more stable estimates at the boundaries.</p>
</div>
<div id="choosing-the-number-and-locations-of-the-knots" class="section level4">
<h4><span class="header-section-number">4.1.5.6</span> Choosing the number and locations of the knots</h4>
<p>Regression splines are most flexible in regions that contain a lot of knots, because in those regions the polynomial coefficients can change rapidly. Hence, it is an option to place more knots in places where we feel the function might vary most rapidly, and fewer where it seems more stable. However, in practice it is more common to place knots in a uniform fashion. One can specify the desired degrees of freedom and have the software automatically place the corresponding number of knots at uniform quantiles of the data.</p>
<ul>
<li>Natural spline with four degrees of freedom and three knots (<span class="math inline">\(25^{th}, 50^{th}\)</span> and <span class="math inline">\(75^{th}\)</span> percentile) - there are actually five knots (also the boundary knots). A cubic spline with five knots would have 9 degrees of freedom, but natural cubic splines have two additional natural constraints at each of the boundaries to enforce linearity, resulting in <span class="math inline">\(9-4=5\)</span> degrees of freedom. One of these <span class="math inline">\(5\)</span> df gets absorbed by the intercept, and hence, we have <span class="math inline">\(4\)</span> df.</li>
</ul>
<p>To determine the number of knots, one could simply assess which curve looks best. A somewhat more objective approach is to use cross-validation, to determine the number of knots that fit the data best.</p>
<p>Regression splines often give superior results to polynomial regression, because unlike polynomials, which must use a high degree to produce flexible fits, splines introduce flexibility by increasing the number of knots, but keeping the degree fixed. Generally, splines produce more stable estimates. Splines also allow to place more knots, and hence flexibility, over regions where the function <span class="math inline">\(f\)</span> seems to be changing more rapidly, and fewer knots where <span class="math inline">\(f\)</span> is more stable.</p>
</div>
<div id="smoothing-splines" class="section level4">
<h4><span class="header-section-number">4.1.5.7</span> Smoothing splines</h4>
<p>We want to find some function <span class="math inline">\(g(x)\)</span> that fits the observed data well (small RSS) but that also is smooth. A natural approach is to find the fundtion <span class="math inline">\(g\)</span> that minimizes
<span class="math display">\[
\sum^n_{i=1} (y_i - g(x_i))^2 + \lambda \int g&#39;&#39;(t)^2 dt
\]</span>
where <span class="math inline">\(\lambda\)</span> is a non-negative tuning parameter. The function <span class="math inline">\(g\)</span> that minimizes this equation is known as a smoothing spline. The first term in this equation is a loss function that encourages <span class="math inline">\(g\)</span> to fit the data well, and the second term is a penalty that penalizes the variability in <span class="math inline">\(g\)</span>. This is a measure of the total change in the function <span class="math inline">\(g&#39;(t)\)</span> over its entire range. This term encourages <span class="math inline">\(g\)</span> to be smooth. The larger the value of <span class="math inline">\(\lambda\)</span>, the smoother <span class="math inline">\(g\)</span> will be, and so, <span class="math inline">\(\lambda\)</span> controls the bias-variance trade-off. The function <span class="math inline">\(g(x)\)</span> that minimizes the above equation can be shown to have some special properties: it is a piecewise cubic polynomial with knots at the unique values of <span class="math inline">\(x, \dots, x_n\)</span> with continuous first and second derivatives at each knot. Furthermore, it is linear in the region outside of the extreme knots. However, it is not the same natural cubic spline that the basis function approach with knots at <span class="math inline">\(x, \dots, x_n\)</span> would give, but a shrunken version, where <span class="math inline">\(\lambda\)</span> controls the level of shrinkage.</p>
</div>
<div id="choosing-lambda" class="section level4">
<h4><span class="header-section-number">4.1.5.8</span> Choosing lambda</h4>
<p>Usually, degrees of freedom refer to the number of free parameters. Although a smoothing spline has <span class="math inline">\(n\)</span> parameters and hence <span class="math inline">\(n\)</span> nominal degrees of freedom, these <span class="math inline">\(n\)</span> parameters are heavily constrained or shrunk down. Hence, <span class="math inline">\(df_{\lambda}\)</span> is a measure of the flexibility of the smoothing spline; the higher it is, the more flexible it is. The best value of <span class="math inline">\(\lambda\)</span> in terms of RSS can be assessed using cross-validation.</p>
</div>
<div id="local-regression" class="section level4">
<h4><span class="header-section-number">4.1.5.9</span> Local regression</h4>
<p>Computing the fit at some target point <span class="math inline">\(x_0\)</span> using only the nearby training observations.</p>
<ol style="list-style-type: decimal">
<li>Gather the fraction <span class="math inline">\(s = \frac{k}{n}\)</span> of the training points whose <span class="math inline">\(x_i\)</span> are closest to <span class="math inline">\(x_0\)</span>.</li>
<li>Assign a weight <span class="math inline">\(K_{i0} = K(x_i, x_0)\)</span> to each point in this neighborhood, so that the point furthest from <span class="math inline">\(x_0\)</span> has weight zero, and the closest has the highest weight. All but these <span class="math inline">\(k\)</span> nearest neighbors get weight zero.</li>
<li>Fit a weighted least squares regression using the weights, by finding <span class="math inline">\(\hat{\beta}_0\)</span> and <span class="math inline">\(\hat{\beta}_1\)</span> that minimize
<span class="math display">\[
\sum^n_{i=1} K_{i0}(y_i - \beta_0 - \beta_1x_i)^2.
\]</span></li>
<li>The fitted value at <span class="math inline">\(x_0\)</span> is given by <span class="math inline">\(\hat{f}(x_0)=\hat{\beta} + \hat{\beta_1}x_0\)</span>.</li>
</ol>
<p>In order to perform local regression, there are a number of choices to be made, such as how to define the weighting function <span class="math inline">\(K\)</span>, and whether to fit a linear, quadratic or cubic regression. The most important choice is the span <span class="math inline">\(s\)</span>, that controls the flexibility of the non-linear fit. We can specify <span class="math inline">\(s\)</span> directly, or use cross-validation.</p>
<p>In a setting with multiple features <span class="math inline">\(X_1, \dots, X_p\)</span>, a useful generalization involves fitting a multiple linear regression model that is global in some variables and local in other, more varying variables. However, local regression can perform poorly if <span class="math inline">\(p\)</span> is much larger than <span class="math inline">\(3/4\)</span>, because there are likely to be little nearby observations at <span class="math inline">\(x_0\)</span>.</p>
</div>
<div id="generalized-additive-models" class="section level4">
<h4><span class="header-section-number">4.1.5.10</span> Generalized additive models</h4>
<p>GAMs provide a general framework for extending a linear model by allowing non-linear functions of each of the variables, while maintaining additivity. A natural way to extend the multiple linear regression model in order to allow for non-linear relationships between each feature and the response is to replace each linear component <span class="math inline">\(\beta_jx_{ij}\)</span> with a smooth non-linear function <span class="math inline">\(f_j(x_{ij})\)</span>, by writing the model as
<span class="math display">\[
y_i = \beta_0 + \sum^p_{j=1} f_j(x_{ij}) + \epsilon_i.
\]</span>
This is called an additive model, because we calculate a separate <span class="math inline">\(f_j\)</span> for each <span class="math inline">\(X_j\)</span> and then add together all of their contributions.</p>
<p>Pros and cons of GAMS</p>
<ul>
<li>Pro: GAMs allow us to fit a non-linear <span class="math inline">\(f_j\)</span> to each <span class="math inline">\(X_j\)</span>, so that we can model non-linear relationships automatically that standard linear regression will miss. So, we do not need to manually try out many different transformations on each variable individually.</li>
<li>Pro: Potential to make more accurate predictions for <span class="math inline">\(Y\)</span>.</li>
<li>Pro: Additivity - we can still examine the effects of each <span class="math inline">\(X_j\)</span> on <span class="math inline">\(Y\)</span> individually while holding all other variables fixed.</li>
<li>Pro: The smoothness of the function <span class="math inline">\(f_j\)</span> for the variable <span class="math inline">\(X_j\)</span> can be summarized via degrees of freedom.</li>
<li>Con: With many variables, important interactions can be missed, due to the additivity. However, these can be added manually.</li>
</ul>
</div>
<div id="r4ds---models" class="section level4">
<h4><span class="header-section-number">4.1.5.11</span> R4DS - Models</h4>
<p>Each observation can be used either for exploration or for confirmation, not for both. You can use an observation as many times as you want for exploration, but only once for confirmation. To confirm a hypothesis, you must use data independent of the data you used to generate the hypothesis.</p>
<p>There are two distinct parts of model building:</p>
<ol style="list-style-type: decimal">
<li>Define a family of models that express a precise but generic pattern that you want to capture (a linear or a quadratic function).</li>
<li>Generate a fitted model by finding the model from the family that is the closest to your data.</li>
</ol>
</div>
</div>
</div>
<div id="lecture---regression-i" class="section level2">
<h2><span class="header-section-number">4.2</span> Lecture - Regression I</h2>
<p><span class="math display">\[ y = f(x) + \epsilon \]</span></p>
<p>There are usually a bunch of <span class="math inline">\(X&#39;s\)</span>, in a vector of <span class="math inline">\(p\)</span> predictors. The <span class="math inline">\(f(x)\)</span> is the prediction function that is to be estimated. Also, there are different goals of regression:</p>
<ol style="list-style-type: decimal">
<li>Prediction - given <span class="math inline">\(x\)</span>, work out <span class="math inline">\(f(x)\)</span>.</li>
<li>Inference - is <span class="math inline">\(x\)</span> related to <span class="math inline">\(y\)</span>; how is <span class="math inline">\(x\)</span> related to <span class="math inline">\(y\)</span>; how precise are parameters of <span class="math inline">\(f(x)\)</span> estimated from the data.</li>
</ol>
<p>K-nearest neighbors - we take a neighborhood of points around <span class="math inline">\(x_0\)</span> and predict its average.
<span class="math display">\[
\hat{f}(x) = \frac{1}{n} \sum^n_{i=1} (y|x \in \text{neighborhood}(x))
\]</span>
KNN is intuitive and can work well with not too many predictors, but when there are many (say, 5 or more) predictors, KNN breaks down because the closest points on tens of predictors simultaneously may actually be quite far away (curse of dimensionality).</p>
<div id="model-accuracy-1" class="section level3">
<h3><span class="header-section-number">4.2.1</span> Model accuracy</h3>
<p>The predictions of <span class="math inline">\(\hat{y}\)</span> differ from the true <span class="math inline">\(y\)</span>, and we can evaluate how much this happens on average.</p>
<ul>
<li>Mean squared error: <span class="math inline">\(MSE = \frac{1}{n} \sum^n_{i=1}(y_i - \hat{y}_i)^2\)</span>.</li>
<li>Root mean squared error: <span class="math inline">\(RMSE = \sqrt{MSE}\)</span>.</li>
<li>Mean absolute error: <span class="math inline">\(MAE = \frac{1}{n} \sum^n_{i=1} |y_i - \hat{y}_i|\)</span>.</li>
<li>Median absolute error: <span class="math inline">\(mAE = \text{median}|y - \hat{y}_i|\)</span>.</li>
<li>Proportion of explained variance: <span class="math inline">\(R^2 = \text{correlation}(y,\hat{y})^2\)</span>.</li>
</ul>
<p>In training data, more flexible models <span class="math inline">\(\hat{f}(x)\)</span> may overfit the data, that is, they model the noise in the data.</p>
<p>Unbiased: model that gives the correct prediction, on average, over samples from the target population.</p>
<p>High variance: model that easily overfits accidental patterns.</p>
<p>Modes with more flexibility have less bias, but they also have higher variance. Bias and variance are implicityly linked because they are both affected by model complexity.</p>
<p>Complexity: the amount of variation in the data that is absorbed into the model; the amount of compression performed on the data by the model; the number of effective parameters, relative to the effective degrees of freedom in the data.</p>
<p>So, models with more predictors have a higher complexity, but also models with higher order polynomials and smaller neighborhoods in KNN have more complexity.</p>
<p>Note that the bias-variance trade-off occurs just as much with <span class="math inline">\(n = 100,000,000\)</span> as with <span class="math inline">\(n = 5\)</span>.</p>
<p>The expected value of the MSE equals <span class="math inline">\(E(MSE) = \text{bias}^2 + \text{Variance} + \sigma^2\)</span>, so, the population mean squared error equals the squared bias plus the model variance, plus the irreducible variance. As <span class="math inline">\(K\)</span> in KNN decreases, the model becomes more complex, and the variance increases.</p>
<p>Sometimes, a wrong model is better than a true model (on average).</p>
<p>Or, if you do not believe in true models: sometimes a simple model is better than a more complex one. These factors together determine what works best:</p>
<ul>
<li>How close the functional form of <span class="math inline">\(\hat{f}(x)\)</span> is to the true <span class="math inline">\(f(x)\)</span>;</li>
<li>The amount of irreducible variance (<span class="math inline">\(\sigma^2\)</span>);</li>
<li>The sample size <span class="math inline">\(n\)</span>;</li>
<li>The complexity of the model <span class="math inline">\(p\)</span>, df or equivalent.</li>
</ul>
<p>If you have the true <span class="math inline">\(f(x)\)</span>, you can easily calculate the <span class="math inline">\(E(MSE)\)</span> (this is sometimes called the Bayes error). However, in practice, we do not know the truth. However, we can estimate the truth using the training-validation-test paradigm.</p>
<ul>
<li>Training data - the observations used to fit <span class="math inline">\(\hat{f}(x)\)</span>.</li>
<li>Validation (/dev) data - new observations from the same source as training data (often used several times to select the model complexity).</li>
<li>Test data: New observations from the intended prediction situation.</li>
</ul>
<p>The idea is that the average squared error in the test set <span class="math inline">\(MSE_{\text{test}}\)</span> is an estimate of the Bayes error <span class="math inline">\(E(MSE)\)</span>. However, this only holds when the test set is like the intended prediction situation.</p>
<p>Drawbacks of the train/validation split are that the validation estimate of the test error can be highly variable, depending on precisely which observations are included in the training set and which observations are included in the validation set; in the validation approach, only a subset of the observations - those that are included in the training set rather than in the validation set - are used to fit the model. This suggests that the validation set error may tend to overestimate the test error for the model fit on the entire dataset.</p>
<p>K-fold cross validation is often used to replace the single dev set approach. Namely, one performs the train/dev split several times, and averages the model accuracy. Usually, <span class="math inline">\(K\)</span> is set to equal 5 or 10. When <span class="math inline">\(K = N\)</span>, we have leave-one-out cross-validation. Note that <strong>any procedure that makes decisions based on the data</strong> requires validation. Getting good test data is a difficult problem.</p>
</div>
</div>
<div id="lecture---regression-ii" class="section level2">
<h2><span class="header-section-number">4.3</span> Lecture - Regression II</h2>
<p>In which situations is a parametric model, such as linear regression, better than KNN? When the problem is high-dimensional, it is difficult to find points close to <span class="math inline">\(x\)</span>, also, when the functional form of the model is close to the truth, a parametric model with this functional form is likely to outperform KNN.</p>
<div id="feature-selection-penalization" class="section level3">
<h3><span class="header-section-number">4.3.1</span> Feature selection / penalization</h3>
<p>The bias-variance tradeoff again: more flexibility is good, as it results in less bias; but it is bed in the sense that it results in a higher variance. So what if we made a very flexible model, but told it not to go overboard with the complexity (judging that by validation data)?</p>
<p>There are generally three ways to do so.</p>
<ol style="list-style-type: decimal">
<li>Subset selection: you can buy at most <span class="math inline">\(p\)</span> things - pick the <span class="math inline">\(p\)</span> best predictors of the model (this is called a wrapper); you can only buy the things you like more than <span class="math inline">\(r\)</span> - only pick predictors that correlate more than <span class="math inline">\(r\)</span> with <span class="math inline">\(y\)</span> (this is called a filter).</li>
<li>Shrinkage (penalization, regularization): you can buy what you want, but dont spend more than <span class="math inline">\(\$s\)</span> - keeping the sum of squared (<span class="math inline">\(l_2\)</span>) or absolute (<span class="math inline">\(l_1\)</span>) coefficients below some budget <span class="math inline">\(s\)</span>, for example ridge regression or the lasso (which is called an embedded method).</li>
<li>Dimension reduction (&gt; unsupervised learning) - run an unsupervised model first, then predict <span class="math inline">\(y\)</span> from the resulting <span class="math inline">\(p\)</span> scores.</li>
</ol>
</div>
<div id="wrapper-methods" class="section level3">
<h3><span class="header-section-number">4.3.2</span> Wrapper methods</h3>
<p><strong>The three common algorithms for subset selection</strong></p>
<p>Best subset selection - fit all possible models with at most <span class="math inline">\(p\)</span> predictors and choose the best one (using a metric of choice). The major drawback of this is that this results in <span class="math inline">\(2^p\)</span> possible models, so with a large <span class="math inline">\(p\)</span>, this is not feasible. However, this approach does provide the best model in the set.</p>
<p>Forward stepwise - let <span class="math inline">\(\mathcal{M}_0\)</span> denote the null model, which contains no predictors; then for <span class="math inline">\(k = 0, \dots, p-1\)</span> consider all <span class="math inline">\(p-k\)</span> models that augment the predictors in <span class="math inline">\(\mathcal{M}_k\)</span> with one additional predictor and choose the best among these <span class="math inline">\(p-k\)</span> models, and call it <span class="math inline">\(\mathcal{M}_{k+1}\)</span>; then, select a single best model from <span class="math inline">\(\mathcal{M}_0, \dots, \mathcal{M}_p\)</span>. This approach results in a much more reasonable number of models to estimate.</p>
<p>Backward stepwise - let <span class="math inline">\(\mathcal{M}_p\)</span> denote the full model, which contains all <span class="math inline">\(p\)</span> predictors; then, for <span class="math inline">\(k = p, p-1, \dots, 1\)</span> consider all <span class="math inline">\(k\)</span> models that contain <span class="math inline">\(k-1\)</span> predictors in <span class="math inline">\(\mathcal{M}_k\)</span> and choose the best among these <span class="math inline">\(k\)</span> models, and call it <span class="math inline">\(\mathcal{M}_{k-1}\)</span>; then select a single best model from among <span class="math inline">\(\mathcal{M}_0, \dots, \mathcal{M}_p\)</span>, which also results in a reasonable number of models to consider.</p>
<p><strong>Wrapper feature selection - pros and cons</strong></p>
<p>Best subset selection is an exhaustive search - it finds the best subset, as advertised - when there is enough data to find it; however, it also fits an enormous number of models when <span class="math inline">\(p\)</span> is large, and it is likely that you will run out of validation data quickly.</p>
<p>Forward / backward selection are both much more efficient; however, it is a greedy search that does not guarantees to find the best subset.</p>
<p>Forward versus backward: forward is usually more efficient; sometimes backward is not even possible (when <span class="math inline">\(p &gt; n\)</span>); however, forward can be fooled, especially when two variables work together but do nothing alone, while backward selection considers the performance of variables together with others. However, both backward and forward are well-known to be bad at finding the true subset of predictors, but this does not matter that much when we are only after prediction.</p>
</div>
<div id="filter-methods" class="section level3">
<h3><span class="header-section-number">4.3.3</span> Filter methods</h3>
<p>Univariate filters - select the variables with the highest correlations with <span class="math inline">\(y\)</span>; or all predictors with a correlation above a certain threshold <span class="math inline">\(r\)</span> (also, measures as the <span class="math inline">\(p\)</span>-value, MDL or mutual information can be considered).</p>
</div>
<div id="embedded-methods" class="section level3">
<h3><span class="header-section-number">4.3.4</span> Embedded methods</h3>
<p>Regularization - buying coefficients on a budget.</p>
<p>The algorithm wants to fit the training data, by buying coefficients at the cost of variance - penalize selecting too many coefficients. Regularization is an extremely efficient way to approximately solve the best subset problem, and it often yields very good results.</p>
<p>Ordinary least squares regression finds the <span class="math inline">\(\beta_j\)</span> that minimizes
<span class="math display">\[
MSE = \frac{1}{n} \sum^n_{i=1}(y_i - \hat{i})^2.
\]</span></p>
<p>Penalized (regularized regression) finds the <span class="math inline">\(\beta_j\)</span> that minimizes
<span class="math display">\[
\begin{align}
MSE &amp;= \frac{1}{n} \sum^n_{i=1}(y_i - \hat{y}_i)^2 + \lambda \cdot \text{Penalty} \\ 
&amp;= \frac{1}{n}\sum^n_{i=1} (y_i - \beta_0 -X\beta)^2 + \lambda \sum^p_{j=1}\beta_j^2 \\
\text{or} \\
&amp;= \frac{1}{n}\sum^n_{i=1} (y_i - \beta_0 - X\beta)^2 + \lambda \sum^p_{j=1}|\beta_j|.
\end{align}
\]</span></p>
<p>We can see the penalties as a budget of coefficients. The lasso penalizes in such a way that the coefficients shrink to exactly 0, while ridge regression shrinks the estimates toward 0, but not to exactly 0.</p>
<p>In R:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="reg.html#cb1-1"></a><span class="co">## LASSO</span></span>
<span id="cb1-2"><a href="reg.html#cb1-2"></a>fit &lt;-<span class="st"> </span><span class="kw">glmnet</span>(x, y, <span class="dt">alpha =</span> <span class="dv">1</span>, lambda)</span>
<span id="cb1-3"><a href="reg.html#cb1-3"></a><span class="co">## Ridge regression</span></span>
<span id="cb1-4"><a href="reg.html#cb1-4"></a>fit &lt;-<span class="st"> </span><span class="kw">glmnet</span>(x, y, <span class="dt">alpha =</span> <span class="dv">0</span>, lambda)</span></code></pre></div>
<p>The value for <span class="math inline">\(\lambda\)</span>, which determines the weight of the penalty, can be determined using cross-validation. The usual least squares solution has a <span class="math inline">\(\lambda\)</span> that equals 0. With a higher <span class="math inline">\(\lambda\)</span>, there is a stricter penalty and thus a smaller budget <span class="math inline">\(s\)</span>; so that there is more shrinkage.</p>
<p>Selecting lambda with cross-validation</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb2-1"><a href="reg.html#cb2-1"></a>cvfit &lt;-<span class="st"> </span><span class="kw">cv.glmnet</span>(x, y, <span class="dt">alpha =</span> <span class="dv">1</span>)</span></code></pre></div>
</div>
</div>
<div id="lecture---regression-iii" class="section level2">
<h2><span class="header-section-number">4.4</span> Lecture - Regression III</h2>
<p>Regular linear regression interpretation - one-unit increase in <span class="math inline">\(x\)</span> is followed by a <span class="math inline">\(\beta\)</span> increase in <span class="math inline">\(y\)</span>.
Log-log transformation - 1 increase in <span class="math inline">\(log(x)\)</span> is followed by a <span class="math inline">\(\beta\)</span> increase in <span class="math inline">\(log(y)\)</span>.</p>
<p>By changing features in a smart way, we can change the properties and the interpretation of the linear model. However, the model remains linear in interpretation (a one-unit increase in <span class="math inline">\(x\)</span> is related to a <span class="math inline">\(\beta\)</span> increase in <span class="math inline">\(y\)</span>); but now <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> are different.</p>
<div id="polynomial-regression-1" class="section level3">
<h3><span class="header-section-number">4.4.1</span> Polynomial regression</h3>
<p>Add polynomial terms <span class="math inline">\(x^2, x^3\)</span> etc. to the model. We can also use the function <code>poly</code> in R (i.e., <code>lm(y ~ poly(x, 4))</code>). This results in curved (i.e., nonlinear) predictions.</p>
</div>
<div id="piecewise-regression" class="section level3">
<h3><span class="header-section-number">4.4.2</span> Piecewise regression</h3>
<p>The idea is to cut <span class="math inline">\(x\)</span> into <span class="math inline">\(K\)</span> bins, and estimate an intercept for each bin (using <code>cut</code> in R). This also leads to nonlinear predictions.</p>
</div>
<div id="basis-functions-1" class="section level3">
<h3><span class="header-section-number">4.4.3</span> Basis functions</h3>
<p>Polynomial regression and piecewise regression are examples of the basis function approach:
<span class="math display">\[
y_i = \beta_0 + \beta_1 b_1(x_i) +\beta_2 b_2(x_i) + \dots + \beta_K b_K(x_i) + \epsilon_i,
\]</span>
where the basis functions <span class="math inline">\(b_k(\cdot)\)</span> are fixed and known.</p>
</div>
<div id="splines" class="section level3">
<h3><span class="header-section-number">4.4.4</span> Splines</h3>
<p>Linear spline
<span class="math display">\[
\begin{align}
y_i &amp;= \beta_0 \cdot 1 + \beta_1 \cdot b_1(x_i) + \beta_2 \cdot b_2 (x_i) + \epsilon_i \\
b_1(x_i) &amp;= x_i \\
b_2(x_i) &amp;= (x_i - \xi)_+
\end{align}
\]</span>
where <span class="math inline">\((\cdot)_+\)</span> means the positive part, so
<span class="math display">\[
(x_i-50)_+ =
\begin{cases}
x_i - 50, &amp; \text{if} ~ x &lt; 50 \\
0, &amp; \text{otherwise}
\end{cases}.
\]</span></p>
<p>The regression coefficients of a linear spline can be interpreted as <span class="math inline">\(\beta_1\)</span> is the regular linear regression slope, and <span class="math inline">\(\beta_2\)</span> is the change in slope after the knot <span class="math inline">\(\xi\)</span>. When doing piecewise cubic regression, there are 8 parameters fitted, including the intercept. So, the intercept, the regression coefficients for <span class="math inline">\(x_i, x_i^2, x_i^3\)</span>, the intercept at the knot and the three regression coefficients after the knot. A regular cubic spline only fits five parameters - the intercept, the regression coefficients for <span class="math inline">\(x_i, x_i^2, x_i^3\)</span> and the regression coefficient after the knot <span class="math inline">\((x_i - \xi)^3_+\)</span>. In R, we can use the spline package, with the functions <code>bs(x, knots = k, degree = d)</code> within <code>lm</code>.</p>
<p>Note that the number of knots in splines can be optimized using a cross-validation loop. Also, the location of the knots <span class="math inline">\((\xi_1, \xi_2, \dots, \xi_K)\)</span> is a hyperparameter, but this one usually divides the data in bins of equal numbers of observations (quantiles). The number and location of the knots depend on the data. There are also natural cubic splines, these are required to be linear at the boundary, and can be used using the function <code>ns(x, df = ...)</code>.</p>
</div>
<div id="local-regression-1" class="section level3">
<h3><span class="header-section-number">4.4.5</span> Local regression</h3>
<p>Local regression is an extension to K-nearest neighbors in two ways: (1) it weights the neighbouring points by some function, the kernel <span class="math inline">\(K(\cdot, \cdot)\)</span>; (2) it determines not only the interept, but also the slope and possible higher-order polynomials. Common kernels are uniform kernels, the Epanechnikov kernel and the tricube kernel. However, for local regression, the same holds as for K-nearest neighbor regression, namely that if the number of predictors becomes too large, KNN breaks down because the closest points on tens of predictors simultaneously may actually be quite far away (curse of dimensionality).</p>
</div>
</div>
<div id="practical-regression-i" class="section level2">
<h2><span class="header-section-number">4.5</span> Practical Regression I</h2>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="ggplot2.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="applications.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/03-linreg.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["dav.pdf", "dav.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
